{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77f228e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-27 22:05:36.505394: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-27 22:05:38.309189: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import tensorflow\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_recall_fscore_support, classification_report, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575f2ee7",
   "metadata": {},
   "source": [
    "## Import the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1ca0a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat_eng = pd.read_csv('df_feature_engineering.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d76f47d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete = pd.read_csv('final_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e99179",
   "metadata": {},
   "source": [
    "## Import the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c28713f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-27 22:05:40.171250: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-27 22:05:40.172212: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "NN_feat_eng = load_model('NN_feat_eng.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "095740bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "NN_full_data = load_model('NN_full_data.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "853f16a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "NN_feat_eng_scaled = load_model('NN_feat_eng_scaled.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b27e5e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "NN_full_data_scaled = load_model('NN_full_data_scaled.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d6d0930",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('xgboost_model.pkl', 'rb') as file:\n",
    "    # Code to read from the file in binary mode\n",
    "    xgboost_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9096e991",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('random_forest_model.pkl', 'rb') as file:\n",
    "    # Code to read from the file in binary mode\n",
    "    random_forest_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e3e3f7",
   "metadata": {},
   "source": [
    "## Defining the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92b3495a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat_eng_target = df_feat_eng['Rain Tomorrow']\n",
    "df_feat_eng_inputs = df_feat_eng.drop(columns = ['Rain Tomorrow'])\n",
    "\n",
    "df_complete_target = df_complete['Rain Tomorrow']\n",
    "df_complete_inputs = df_complete.drop(columns = ['Rain Tomorrow'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0de82bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val_complete, X_test_complete, y_train_val_complete, y_test_complete = train_test_split(df_complete_inputs.values, df_complete_target.values, test_size=0.1, random_state=42)\n",
    "X_train_val_feat_eng, X_test_feat_eng, y_train_val_feat_eng, y_test_feat_eng = train_test_split(df_feat_eng_inputs.values, df_feat_eng_target.values, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c458b3e1",
   "metadata": {},
   "source": [
    "## Defining the scaled test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e50f586f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = MinMaxScaler().fit(df_feat_eng_inputs)\n",
    "\n",
    "x_feat_eng_scaled = pd.DataFrame(transformer.transform(df_feat_eng_inputs), columns = df_feat_eng_inputs.columns)\n",
    "y_feat_eng_scaled = df_feat_eng_target\n",
    "\n",
    "# Split into training+validation and testing sets (90% train+val, 10% test)\n",
    "X_train_val, X_test_feat_eng_scaled, y_train_val, y_test_feat_eng_scaled = train_test_split(x_feat_eng_scaled.values, y_feat_eng_scaled.values, test_size=0.1, random_state=42)\n",
    "# Split training+validation set into training and validation sets (of the 90% for training and validation, 90*0.21 = 12% is validation and 68% is training )\n",
    "X_train_feat_eng_scaled, X_val_feat_eng_scaled, y_train_feat_eng_scaled, y_val_feat_eng_scaled = train_test_split(X_train_val, y_train_val, test_size=0.21, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96d57cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = MinMaxScaler().fit(df_complete_inputs)\n",
    "\n",
    "x_complete_scaled = pd.DataFrame(transformer.transform(df_complete_inputs), columns = df_complete_inputs.columns)\n",
    "y_complete_scaled = df_complete_target\n",
    "\n",
    "# Split into training+validation and testing sets (90% train+val, 10% test)\n",
    "X_train_val, X_test_complete_scaled, y_train_val, y_test_complete_scaled = train_test_split(x_complete_scaled.values, y_complete_scaled.values, test_size=0.1, random_state=42)\n",
    "# Split training+validation set into training and validation sets (of the 90% for training and validation, 90*0.21 = 19% is validation and 71% is training )\n",
    "X_train_complete_scaled, X_val_complete_scaled, y_train_complete_scaled, y_val_complete_scaled = train_test_split(X_train_val, y_train_val, test_size=0.21, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b94d0dc",
   "metadata": {},
   "source": [
    "## Making the predictions for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "721340d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Store the values\n",
    "predictions_complete = (NN_full_data.predict(X_test_complete) > 0.5).astype(\"int32\")\n",
    "predictions_feat_eng = (NN_feat_eng.predict(X_test_feat_eng) > 0.5).astype(\"int32\")\n",
    "predictions_feat_eng_scaled = (NN_feat_eng_scaled.predict(X_test_feat_eng_scaled) > 0.5).astype(\"int32\")\n",
    "predictions_complete_scaled = (NN_full_data_scaled.predict(X_test_complete_scaled) > 0.5).astype(\"int32\")\n",
    "predictions_rf = random_forest_model.predict(X_test_feat_eng)\n",
    "predictions_xgb = xgboost_model.predict(X_test_feat_eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d57b93",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "acdb2a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Confusion Matrix for the NN_full_data\n",
      "\n",
      "                  No rain tomorrow  Rains Tomorrow\n",
      "No rain tomorrow             10308             418\n",
      "Rains Tomorrow                1695            1286\n",
      "\n",
      "Accuracy:  0.8458451885897716\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.96      0.91     10726\n",
      "           1       0.75      0.43      0.55      2981\n",
      "\n",
      "    accuracy                           0.85     13707\n",
      "   macro avg       0.81      0.70      0.73     13707\n",
      "weighted avg       0.84      0.85      0.83     13707\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Test Confusion Matrix for the NN_full_data')\n",
    "\n",
    "print()\n",
    "print(pd.DataFrame(confusion_matrix(y_test_complete, predictions_complete), index=['No rain tomorrow', 'Rains Tomorrow'], columns=['No rain tomorrow', 'Rains Tomorrow']))\n",
    "\n",
    "print('\\nAccuracy: ',accuracy_score(y_test_complete, predictions_complete))\n",
    "print()\n",
    "print(classification_report(y_test_complete, predictions_complete))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f3e2f3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Confusion Matrix for the NN_full_data_scaled\n",
      "\n",
      "                  No rain tomorrow  Rains Tomorrow\n",
      "No rain tomorrow             10186             540\n",
      "Rains Tomorrow                1532            1449\n",
      "\n",
      "Accuracy:  0.8488363609834391\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.95      0.91     10726\n",
      "           1       0.73      0.49      0.58      2981\n",
      "\n",
      "    accuracy                           0.85     13707\n",
      "   macro avg       0.80      0.72      0.75     13707\n",
      "weighted avg       0.84      0.85      0.84     13707\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Test Confusion Matrix for the NN_full_data_scaled')\n",
    "print()\n",
    "print(pd.DataFrame(confusion_matrix(y_test_complete_scaled, predictions_complete_scaled), index=['No rain tomorrow', 'Rains Tomorrow'], columns=['No rain tomorrow', 'Rains Tomorrow']))\n",
    "\n",
    "print('\\nAccuracy: ',accuracy_score(y_test_complete_scaled, predictions_complete_scaled))\n",
    "print()\n",
    "print(classification_report(y_test_complete_scaled, predictions_complete_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d4b18489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Confusion Matrix for the NN_feat_eng\n",
      "\n",
      "                  No rain tomorrow  Rains Tomorrow\n",
      "No rain tomorrow              2301             665\n",
      "Rains Tomorrow                 708            2360\n",
      "\n",
      "Accuracy:  0.7724560822008618\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.78      0.77      2966\n",
      "           1       0.78      0.77      0.77      3068\n",
      "\n",
      "    accuracy                           0.77      6034\n",
      "   macro avg       0.77      0.77      0.77      6034\n",
      "weighted avg       0.77      0.77      0.77      6034\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Test Confusion Matrix for the NN_feat_eng')\n",
    "print()\n",
    "print(pd.DataFrame(confusion_matrix(y_test_feat_eng, predictions_feat_eng), index=['No rain tomorrow', 'Rains Tomorrow'], columns=['No rain tomorrow', 'Rains Tomorrow']))\n",
    "\n",
    "print('\\nAccuracy: ',accuracy_score(y_test_feat_eng, predictions_feat_eng))\n",
    "print()\n",
    "print(classification_report(y_test_feat_eng, predictions_feat_eng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4b528448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Confusion Matrix for the NN_feat_eng_scaled\n",
      "\n",
      "                  No rain tomorrow  Rains Tomorrow\n",
      "No rain tomorrow              2304             662\n",
      "Rains Tomorrow                 722            2346\n",
      "\n",
      "Accuracy:  0.770633079217766\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.78      0.77      2966\n",
      "           1       0.78      0.76      0.77      3068\n",
      "\n",
      "    accuracy                           0.77      6034\n",
      "   macro avg       0.77      0.77      0.77      6034\n",
      "weighted avg       0.77      0.77      0.77      6034\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Test Confusion Matrix for the NN_feat_eng_scaled')\n",
    "print()\n",
    "print(pd.DataFrame(confusion_matrix(y_test_feat_eng_scaled, predictions_feat_eng_scaled), index=['No rain tomorrow', 'Rains Tomorrow'], columns=['No rain tomorrow', 'Rains Tomorrow']))\n",
    "\n",
    "\n",
    "print('\\nAccuracy: ',accuracy_score(y_test_feat_eng_scaled, predictions_feat_eng_scaled))\n",
    "print()\n",
    "print(classification_report(y_test_feat_eng_scaled, predictions_feat_eng_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5d9b97",
   "metadata": {},
   "source": [
    "# Neural networks conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc0b8b5",
   "metadata": {},
   "source": [
    "Comparing the full data and the feature engineering NN's, one can see that the full data has some better stats, like accuracy but it is much worse at predicting correctly when it will rain. The full data NN even has more false positives then true positives, as opposed to the featured engineered.\n",
    "\n",
    "The feature engineering NN's have a much better precision, recall and f1-score for when it rains in the following day, which is the main priority. The scaled and not scaled featured engineering NN's are really similar, although the non scaled one has a slightly more balanced performance, and slightly better accuracy, but in reality it is such a small difference that is not countable.\n",
    "\n",
    "For the purpose of the problem, the NN's that deal with the featured engineered data have the better performance, being clear that the feature engineering part was well done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570d7ad4",
   "metadata": {},
   "source": [
    "## Ensemble methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a8f0ac7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Confusion Matrix for the random forest\n",
      "\n",
      "                  No rain tomorrow  Rains Tomorrow\n",
      "No rain tomorrow              2295             671\n",
      "Rains Tomorrow                 706            2362\n",
      "\n",
      "Accuracy:  0.7717931720251906\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.77      0.77      2966\n",
      "           1       0.78      0.77      0.77      3068\n",
      "\n",
      "    accuracy                           0.77      6034\n",
      "   macro avg       0.77      0.77      0.77      6034\n",
      "weighted avg       0.77      0.77      0.77      6034\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Test Confusion Matrix for the random forest')\n",
    "\n",
    "print()\n",
    "print(pd.DataFrame(confusion_matrix(y_test_feat_eng, predictions_rf), index=['No rain tomorrow', 'Rains Tomorrow'], columns=['No rain tomorrow', 'Rains Tomorrow']))\n",
    "\n",
    "print('\\nAccuracy: ',accuracy_score(y_test_feat_eng, predictions_rf))\n",
    "print()\n",
    "print(classification_report(y_test_feat_eng, predictions_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "476243bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Confusion Matrix for the xgboost model\n",
      "\n",
      "                  No rain tomorrow  Rains Tomorrow\n",
      "No rain tomorrow              2308             658\n",
      "Rains Tomorrow                 713            2355\n",
      "\n",
      "Accuracy:  0.7727875372886974\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.78      0.77      2966\n",
      "           1       0.78      0.77      0.77      3068\n",
      "\n",
      "    accuracy                           0.77      6034\n",
      "   macro avg       0.77      0.77      0.77      6034\n",
      "weighted avg       0.77      0.77      0.77      6034\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Test Confusion Matrix for the xgboost model')\n",
    "print()\n",
    "print(pd.DataFrame(confusion_matrix(y_test_feat_eng, predictions_xgb), index=['No rain tomorrow', 'Rains Tomorrow'], columns=['No rain tomorrow', 'Rains Tomorrow']))\n",
    "\n",
    "\n",
    "print('\\nAccuracy: ',accuracy_score(y_test_feat_eng, predictions_xgb))\n",
    "print()\n",
    "print(classification_report(y_test_feat_eng, predictions_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcd2d1f",
   "metadata": {},
   "source": [
    "# Ensemble methods conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc4ec88",
   "metadata": {},
   "source": [
    "It is possible to see that both the random forest and the xgboost models have virtually equal performances. The differences, like in the NN's, are so little that they are not really countable as well. They are equally good at predicting when it will or won't rain, being slightly better at predicting when it will rain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8312f4",
   "metadata": {},
   "source": [
    "# Models conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d562917",
   "metadata": {},
   "source": [
    "Both the ensemble methods and the feature engineering NN's have really similar performances although the factor that the ensemble methods take much longer to be trained has to be considered. The random forest model took 1.5 hours for the best model to be found, the XGBoost took 3 hours (it also searched for less number of estimators), the full data NN took 20 minutes and the featuring engineering NN took 10 minutes. Ironically, the best model took the less time to be trained. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
